{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation of Search Activity on Commons\n",
    "\n",
    "Per [T258229](https://phabricator.wikimedia.org/T258229), we aggregate daily measurements of search activity on Commons. In this notebook, we gather statistics for both legacy search and Special:MediaSearch. Specifically, we gather data on the following six measurements:\n",
    "\n",
    "* Number of search sessions\n",
    "* Number of searches made\n",
    "* Number of searches per session\n",
    "* Search session length\n",
    "* Click-through rate (to quickview and from quickview to file pages)\n",
    "* Average position of clicked result in successful searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from wmfdata import spark, mariadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Timestamps\n",
    "\n",
    "We'll call the day we're gathering data for `data_day`. We're also expecting this notebook to be run the day after, which we'll call `next_day`. In order to ignore search sessions that started on the previous day, we also define that day. Lastly, we set a limit of one hour after midnight UTC as the cutoff for data. In other words, we expect search sessions to be completed within one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_day = dt.datetime.now(dt.timezone.utc).date()\n",
    "\n",
    "data_day = next_day - dt.timedelta(days = 1)\n",
    "previous_day = data_day - dt.timedelta(days = 1)\n",
    "\n",
    "limit_timestamp = dt.datetime.combine(next_day, dt.time(hour = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Tables\n",
    "\n",
    "We define a set of tables in the Data Lake for aggregation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_count_table_name = 'wmf_product.commons_search_counts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_search_count_query = '''\n",
    "CREATE TABLE {table_name} (\n",
    "    log_date DATE COMMENT \"the date of the aggregated search counts\",\n",
    "    wiki STRING COMMENT \"the project name of search counts\",\n",
    "    num_legacy_sessions BIGINT COMMENT \"the number of legacy search sessions\",\n",
    "    num_autocomplete_searches BIGINT COMMENT \"the number of autocomplete searches\",\n",
    "    num_fulltext_successful_searches BIGINT COMMENT \"the number of fulltext searches with results\",\n",
    "    num_fulltext_zeroresult_searches BIGINT COMMENT \"the number of fulltext searches with no results\",\n",
    "    median_autocomp_searches_per_session DOUBLE COMMENT \"median number of autocomplete searches per sesssion\",\n",
    "    median_ft_success_searches_per_session DOUBLE COMMENT \"median number of fulltext searches with results per session\",\n",
    "    median_ft_zero_searches_per_session DOUBLE COMMENT \"median number of fulltext searches with no results per session\",\n",
    "    median_legacy_session_length DOUBLE COMMENT \"median length of a search session, in seconds\",\n",
    "    num_mediasearch_sessions BIGINT COMMENT \"the number of MediaSearch search sessions\",\n",
    "    num_mediasearch_searches BIGINT COMMENT \"the number of searches made in MediaSearch sessions\",\n",
    "    median_mediasearch_searches_per_session DOUBLE COMMENT \"median number of searches, per MediaSearch session\",\n",
    "    median_mediasearch_session_length DOUBLE COMMENT \"median length of a MediaSearch session, in seconds\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(create_search_count_query.format(table_name = search_count_table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partition_statement(start_ts, end_ts, prefix = ''):\n",
    "    '''\n",
    "    This takes the two timestamps and creates a statement that selects\n",
    "    partitions based on `year`, `month`, and `day` in order to make our\n",
    "    data gathering not use excessive amounts of data. It assumes that\n",
    "    `start_ts` and `end_ts` are not more than a month apart, which should\n",
    "    be a reasonable expectation for this notebook.\n",
    "    \n",
    "    An optional prefix can be set to enable selecting partitions for\n",
    "    multiple tables with different aliases.\n",
    "    \n",
    "    :param start_ts: start timestamp\n",
    "    :type start_ts: datetime.datetime\n",
    "    \n",
    "    :param end_ts: end timestamp\n",
    "    :type end_ts: datetime.datetime\n",
    "    \n",
    "    :param prefix: prefix to use in front of partition clauses, \".\" is added automatically\n",
    "    :type prefix: str\n",
    "    '''\n",
    "    \n",
    "    if prefix:\n",
    "        prefix = f'{prefix}.' # adds \".\" after the prefix\n",
    "    \n",
    "    # there are three cases:\n",
    "    # 1: month and year are the same, output a \"BETWEEN\" statement with the days\n",
    "    # 2: months differ, but the years are the same.\n",
    "    # 3: years differ too.\n",
    "    # Case #2 and #3 can be combined, because it doesn't really matter\n",
    "    # if the years are the same in the month-selection or not.\n",
    "    \n",
    "    if start_ts.year == end_ts.year and start_ts.month == end_ts.month:\n",
    "        return(f'''{prefix}year = {start_ts.year}\n",
    "AND {prefix}month = {start_ts.month}\n",
    "AND {prefix}day BETWEEN {start_ts.day} AND {end_ts.day}''')\n",
    "    else:\n",
    "        return(f'''\n",
    "(\n",
    "    ({prefix}year = {start_ts.year}\n",
    "     AND {prefix}month = {start_ts.month}\n",
    "     AND {prefix}day >= {start_ts.day})\n",
    " OR ({prefix}year = {end_ts.year}\n",
    "     AND {prefix}month = {end_ts.month}\n",
    "     AND {prefix}day <= {end_ts.day})\n",
    ")''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of searches\n",
    "\n",
    "Note: I did a pull of a week's worth of searches on Commons and found the difference between `meta.dt` and `client_dt` to generally be very small, typically within a minute. Instead of doing time math to ignore sessions with large drifts, we'll instead coalesce the two and ignore sessions that are outside our defined time limitations, which should be a very reasonable decision.\n",
    "\n",
    "The query below limits sessions for legacy search to those having less than 50 searches in them. This is in order to focus on non-automated traffic and is an approach that's been used in search analysis in the past, for example when we gathered baseline metrics for legacy search in [T258723](https://phabricator.wikimedia.org/T258723).\n",
    "\n",
    "Note that we calculate medians (using `percentile()`) as the \"average number of searches made per session\" because we've previously found that search activity has a long-tail distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_count_query = '''\n",
    "WITH legacy_sessions AS ( -- all legacy search sessions started during the day of interest\n",
    "    SELECT\n",
    "        wiki,\n",
    "        event.searchsessionid AS session_id,\n",
    "        MIN(coalesce(client_dt, meta.dt)) AS session_start_dt\n",
    "    FROM event.searchsatisfaction AS ess\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND wiki = \"commonswiki\"\n",
    "    AND useragent.is_bot = false\n",
    "    AND event.subTest IS NULL\n",
    "    AND event.action = \"searchResultPage\"\n",
    "    AND event.isforced IS NULL -- only include non-test users\n",
    "    GROUP BY wiki, event.searchsessionid\n",
    "    HAVING TO_DATE(session_start_dt) = \"{today}\"\n",
    "),\n",
    "legacy_session_end AS ( -- timestamp of last event in a valid legacy search session\n",
    "    SELECT\n",
    "        ls.wiki,\n",
    "        event.searchsessionid AS session_id,\n",
    "        MAX(coalesce(client_dt, meta.dt)) AS session_end_dt\n",
    "    FROM legacy_sessions AS ls\n",
    "    INNER JOIN event.searchsatisfaction AS ess\n",
    "    ON ls.session_id = ess.event.searchsessionid\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND event.action != \"checkin\" -- don't count page checkins where a user is reading a page\n",
    "    AND coalesce(client_dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    GROUP BY ls.wiki,event.searchsessionid\n",
    "),\n",
    "legacy_counts AS ( -- count of searches based on valid legacy search sessions\n",
    "    SELECT\n",
    "        ls.wiki,\n",
    "        event.searchsessionid AS session_id,\n",
    "        COUNT(DISTINCT\n",
    "                IF(event.source = \"autocomplete\", event.searchsessionid, NULL),\n",
    "                IF(event.source = \"autocomplete\", event.pageviewid, NULL)) AS num_autocomplete_searches,\n",
    "        COUNT(IF(event.source = \"fulltext\"\n",
    "            AND event.hitsReturned > 0 , 1, NULL)) AS num_fulltext_successful_searches,\n",
    "        COUNT(IF(event.source = \"fulltext\"\n",
    "            AND event.hitsReturned IS NULL , 1, NULL)) AS num_fulltext_zeroresult_searches\n",
    "    FROM legacy_sessions AS ls\n",
    "    INNER JOIN event.searchsatisfaction AS ess\n",
    "    ON ls.session_id = ess.event.searchsessionid\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND coalesce(client_dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    AND event.action = \"searchResultPage\"\n",
    "    GROUP BY ls.wiki, event.searchsessionid\n",
    "),\n",
    "mediasearch_sessions AS ( -- all MediaSearch sessions started during the day of interest\n",
    "    SELECT\n",
    "        CASE WHEN normalized_host.project = 'commons' THEN 'commonswiki' ELSE CONCAT(normalized_host.project,normalized_host.project_class) END AS wiki,\n",
    "        web_pageview_id AS session_id,\n",
    "        MIN(coalesce(dt, meta.dt)) AS session_start_dt\n",
    "    FROM event.mediawiki_mediasearch_interaction AS ms\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND action = \"search_new\"\n",
    "    AND normalized_host.project IN ('pt', 'commons')\n",
    "    GROUP BY CASE WHEN normalized_host.project = 'commons' THEN 'commonswiki' ELSE CONCAT(normalized_host.project,normalized_host.project_class) END, web_pageview_id\n",
    "    HAVING TO_DATE(session_start_dt) = \"{today}\"\n",
    "),\n",
    "mediasearch_session_end AS ( -- timestamp of last event in a valid Mediasearch session\n",
    "    SELECT\n",
    "        wiki,\n",
    "        web_pageview_id AS session_id,\n",
    "        MAX(coalesce(dt, meta.dt)) AS session_end_dt\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    INNER JOIN event.mediawiki_mediasearch_interaction AS ms\n",
    "    ON mess.session_id = ms.web_pageview_id\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND coalesce(dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    GROUP BY wiki, web_pageview_id\n",
    "),\n",
    "mediasearch_counts AS ( -- count of searches based on valid MediaSearch sessions\n",
    "    SELECT\n",
    "        wiki,\n",
    "        web_pageview_id AS session_id,\n",
    "        COUNT(1) AS num_mediasearch_searches\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    INNER JOIN event.mediawiki_mediasearch_interaction AS ms\n",
    "    ON mess.session_id = ms.web_pageview_id\n",
    "    WHERE {ms_partition_statement}\n",
    "    AND coalesce(dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    AND action = \"search_new\"\n",
    "    GROUP BY wiki, web_pageview_id\n",
    "),\n",
    "legacy_stats AS ( -- statistics for legacy search\n",
    "    SELECT\n",
    "        TO_DATE(session_start_dt) AS log_date,\n",
    "        ls.wiki,\n",
    "        COUNT(1) AS num_legacy_sessions,\n",
    "        SUM(num_autocomplete_searches) AS num_autocomplete_searches,\n",
    "        SUM(num_fulltext_successful_searches) AS num_fulltext_successful_searches,\n",
    "        SUM(num_fulltext_zeroresult_searches) AS num_fulltext_zeroresult_searches,\n",
    "        percentile(\n",
    "            unix_timestamp(session_end_dt, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") -\n",
    "                unix_timestamp(session_start_dt, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"),\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_legacy_session_length,\n",
    "        percentile(\n",
    "            num_autocomplete_searches,\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_autocomp_searches_per_session,\n",
    "        percentile(\n",
    "            num_fulltext_successful_searches,\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_ft_success_searches_per_session,\n",
    "        percentile(\n",
    "            num_fulltext_zeroresult_searches,\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_ft_zero_searches_per_session\n",
    "    FROM legacy_sessions AS ls\n",
    "    INNER JOIN legacy_session_end AS lse\n",
    "    ON (ls.session_id = lse.session_id AND ls.wiki = lse.wiki)\n",
    "    INNER JOIN legacy_counts AS lc\n",
    "    ON (ls.session_id = lc.session_id AND ls.wiki = lc.wiki)\n",
    "    -- cutoff of 50 searches per session to remove automated traffic\n",
    "    WHERE (num_autocomplete_searches + num_fulltext_successful_searches) < 50\n",
    "    GROUP BY TO_DATE(session_start_dt),ls.wiki\n",
    "),\n",
    "mediasearch_stats AS ( -- statistics for MediaSearch\n",
    "    SELECT\n",
    "        TO_DATE(session_start_dt) AS log_date,\n",
    "        mess.wiki,\n",
    "        COUNT(1) AS num_mediasearch_sessions,\n",
    "        SUM(num_mediasearch_searches) AS num_mediasearch_searches,\n",
    "        percentile(\n",
    "            unix_timestamp(session_end_dt, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") -\n",
    "                unix_timestamp(session_start_dt, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"),\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_mediasearch_session_length,\n",
    "        percentile(\n",
    "            num_mediasearch_searches,\n",
    "            0.5 -- median is the 50th percentile\n",
    "        ) AS median_mediasearch_searches_per_session\n",
    "    FROM mediasearch_sessions AS mess\n",
    "    INNER JOIN mediasearch_session_end AS mse\n",
    "    ON (mess.session_id = mse.session_id AND mess.wiki = mse.wiki)\n",
    "    INNER JOIN mediasearch_counts AS mc\n",
    "    ON (mess.session_id = mc.session_id AND mess.wiki = mse.wiki)\n",
    "    GROUP BY TO_DATE(session_start_dt),mess.wiki\n",
    ")\n",
    "\n",
    "INSERT INTO {aggregate_table}\n",
    "SELECT\n",
    "    ms.log_date,\n",
    "    ms.wiki,\n",
    "    coalesce(ls.num_legacy_sessions, 0) AS num_legacy_sessions,\n",
    "    coalesce(ls.num_autocomplete_searches,0) AS num_autocomplete_searches,\n",
    "    coalesce(ls.num_fulltext_successful_searches,0) AS num_fulltext_successful_searches,\n",
    "    coalesce( ls.num_fulltext_zeroresult_searches,0) AS num_fulltext_zeroresult_searches,\n",
    "    coalesce(ls.median_autocomp_searches_per_session,0) AS median_autocomp_searches_per_session,\n",
    "    coalesce(ls.median_ft_success_searches_per_session,0) AS median_ft_success_searches_per_session,\n",
    "    coalesce(ls.median_ft_zero_searches_per_session,0) AS median_ft_zero_searches_per_session,\n",
    "    coalesce(ls.median_legacy_session_length,0) AS median_legacy_session_length,\n",
    "    coalesce(ms.num_mediasearch_sessions, 0) AS num_mediasearch_sessions,\n",
    "    coalesce(ms.num_mediasearch_searches, 0) AS num_mediasearch_searches,\n",
    "    coalesce(ms.median_mediasearch_searches_per_session, 0.0) AS median_mediasearch_searches_per_session,\n",
    "    coalesce(ms.median_mediasearch_session_length, 0.0) AS median_mediasearch_session_length\n",
    "FROM mediasearch_stats AS ms\n",
    "LEFT JOIN legacy_stats  AS ls\n",
    "ON (ls.log_date = ms.log_date AND ls.wiki = ms.wiki)\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(search_count_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        # aggregate_table = table_name\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'`wmf_product`.`commons_search_counts` requires that the data to be inserted have the same number of columns as the target table: target table has 13 column(s) but the inserted data has 14 column(s), including 0 partition column(s) having constant value(s).;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o329.sql.\n: org.apache.spark.sql.AnalysisException: `wmf_product`.`commons_search_counts` requires that the data to be inserted have the same number of columns as the target table: target table has 13 column(s) but the inserted data has 14 column(s), including 0 partition column(s) having constant value(s).;\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(rules.scala:341)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:376)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:368)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.apply(rules.scala:368)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.apply(rules.scala:328)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1923/4239317202.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mess_partition_statement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_partition_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ess'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mms_partition_statement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_partition_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ms'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0maggregate_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_count_table_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     ))\n\u001b[1;32m      9\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mUnboundLocalError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/anaconda-wmf/lib/python3.7/site-packages/wmfdata/spark.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(commands, format, session_type, extra_settings)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommands\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mcmd_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# If the result has columns, the command was a query and therefore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# results-producing. If not, it was a DDL or DML command and not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: '`wmf_product`.`commons_search_counts` requires that the data to be inserted have the same number of columns as the target table: target table has 13 column(s) but the inserted data has 14 column(s), including 0 partition column(s) having constant value(s).;'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.run(search_count_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        aggregate_table = search_count_table_name\n",
    "    ))\n",
    "except UnboundLocalError:\n",
    "    # wmfdata currently (late Feb 2021) has an issue with DDL/DML SQL queries,\n",
    "    # and so we ignore that error\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
